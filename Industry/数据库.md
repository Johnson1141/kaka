### [OceanBase质量之道](https://mp.weixin.qq.com/s/d6mC3EPlPiBAB9R0b70K_g)
1. 测试和开发同时进行：
    1. 建立代码准入流程（OceanBase的编码规范长达几十页），从源端控制引入bug的概率
    2. 让开发人员专注代码本身，避免在测试执行上花费太多时间，OceanBase团队建立了高效轻便的TaaS（即Testing-as-a-Service）平台，在本开发机用快捷命令可以完成大部分日常回归测试，测试任务机器、执行工具等对开发人员透明
    3. obfarm（测试农场）是特定测试用例的集合，包括SQL功能、分布式功能、版本间兼容性等等，通过obfarm的测试用例是代码提交的最低要求
    2. 资深测试人员的精力主要放在难度较大的bug的发现，测试体系建设和相关技术钻研、测试自动化实施
    3. OceanBase有测试机器数百台，它们7*24小时在运行每一个测试任务
2. 设计方面提高质量：
    2. 数据自检：线下测试有80%的数据正确性问题都是通过数据库自检发现的，大大降低了生产环境出现数据不一致后的风险。redolog、不同副本、磁盘数据都有checksum校验。
    3. 灰度发布：大型数据库软件的开发测试周期一般比较长，bug数往往是以万为单位计的，这也是为什么传统数据库厂商每隔1-2年才出一个GA版本的原因之一。OceanBase诞生于互联网公司，必须具备快速迭代快速上线的能力，不可能等所有的功能和bugfix都完成后才发布上线。
3. 测试点：
    1. 异常测试：分布式系统的异常测试，包括网络分区、网络抖动、磁盘异常、程序退出、机器掉电等等。分布式系统状态复杂，组合是爆炸式的，很多bug并不会直接导致进程crash，或者是集群状态紊乱，需要系统7*24h运行、数据规模达到一定级别，随机注入异常(开源工具有fiu、Jepsen、Simian Army等等)，各种状态组合才能将bug暴露出来。
    2. 一致性测试：自研的工具，可以覆盖并发事务（只读、长事务、锁冲突等维度）、主备切换（主动、被动）、系统恢复等生产环境可能碰到的情况，事务的ACID特性都能得到验证。这个工具自问世以来一直是发现高质量bug的利器，为版本质量立下了汗马功劳。

### [何登成13年开发缩影与阿里数据库内核锤炼之路](https://mp.weixin.qq.com/s/nRq7MVPZfjtj3-T_KZOlEg)

#### 双十一
2017年，秒级 32.5 万的交易峰值，团队真正做到了双 11 前 CTO 定下的“喝着茶过双 11”的基调，值班的技术同学也很快的进入了买买买的剁手行列之中。

##### 发现问题
数据库团队来说，并不知道每个集群的压力在双 11 零点峰值会有多大？哪个数据库集群会最先碰到瓶颈？每个集群需要扩容多少机器？双 11 备战时间过长，能不能缩短？双 11 参与的技术人员过多，能不能减少？双 11 的硬件资源开销过大，能不能降低？

##### 技术思路
1. 将双 11 从技术上的一个未知问题转变为一个已知问题，然后每年对这个已知问题进行求解。
2. 当团队将未知问题转化为已知问题之后，就在不断地思考：现有的解决方案是否是最优方案，在技术上是否有改进的空间。

#### 数据库产品线
##### AliSQL
阿里巴巴数据库内核研发团队组建于 2010 年，成立初期的目标就是去 IOE，早期的产品是开源 MySQL 的阿里巴巴分支 AliSQL，对 MySQL 在性能、功能和可运维性等方面，做了大量的改进

##### X-DB
结合 AliSQL 和自研高性能分布式一致性算法 X-Paxos、自研高性能低成本存储引擎 X-Engine，全面兼容 MySQL 生态，数据强同步，自动选主，一跃成为一个达到金融级别可靠性和可用性的数据库。在今年的双 11 核心集群上大规模部署。

* 一体化理念：没有任何第三方的外部组件依赖，对业务实现了透明化的数据 Sharding 能力以及跨 AZ（同城多机房）、Region（跨城部署）甚至是全球化部署的能力，5 个 9 以上的持续可用率。
* 存储计算分离：数据库的计算存储分离本身不难，难的是计算存储分离之后，访问存储层数据的延时增加对性能的负面影响。数据库计算存储分离在2017年双 11 核心集群落地，将数据库从一个重量级的有状态系统转化为一个轻量级的无状态系统，具备了等同于业务系统的弹性能力。
* 兼容成熟生态：对 MySQL 生态下的运维系统 / 工具、知识体系实现了兼容，整个 MySQL 时代的支撑平台和人员都可以平滑的过度到分布式数据库时代，拥有了支撑下一代数据库的能力。

（译者注：mintplus@bd，集成raft去zk体现了一体化，托管PaaS实现混布和副本保活，使用redis协议兼容生态。）

##### X-KV
核心集群在双 11 的读压力非常大，传统的解决方案是在数据库前端搭建缓存。但是增加缓存会带来数据一致性问题同时增加架构复杂性。

将缓存和数据库融合在一起，通过 KV 接口直接访问数据库存储引擎中的数据，一来消除了数据一致性问题，二来绕过了整个数据库的 SQL 层，极大地提升了数据库的读能力。经过验证，通过 KV 接口直接读取数据库，在我们某一个数据库核心集群上，相对于 SQL 接口有着近 5 倍的性能提升。

##### X-Engine
传统架构下的数据库系统，只有 6.8% 的 CPU 资源做的是有用功，而绝大部分的 CPU 都消耗在了 2B3L 上（2B：B+Tree，Buffer Pool；3L：Logging，Locking，Latching）。软件的性能提升越来越赶不上硬件的更新换代，软件系统内部的并发瓶颈极大的限制了系统在多核上的扩展性，往往一台主机上需要部署多个数据库实例，才能将硬件的资源耗尽。

设计理念：在 100% 兼容 MySQL 生态的前提下，面向软硬件 Co-Design，通过更亲和现代硬件的软件架构来释放硬件的效率，让每个 CPU 的周期，每个内外存的 bit，每个网卡的 IO 都产生业务价值。

落地：针对内存以及不同外存的特性分别设计更亲和其硬件特性的数据结构和算法，充分提高多种硬件的效率。例如：在内存和 CPU 缓存层面，X-Engine 实现了非常亲和 CPU 缓存的内存索引结构；在外存层面，X-Engine 对 NVRAM、SSD、HDD 等多种外存硬件采用分层存储，不同的存储层根据硬件特征保存不同数据格式和业务意义的数据，提升存储效率。无锁数据结构，轻量级锁，乐观并发控制实现来减少上下文的切换，进而减少 CPU 的 stall；同时引入 FPGA 等异构计算芯片，将compression、compaction 等矢量计算逻辑下沉到异构计算。将 DPDK、RDMA、用户态协议栈等下一代网络架构融合到自身的分布式层中，提升分布式一致性、分布式存储、分布式事务的效率。

在标准的 Sysbench 纯写、纯读测试场景下，32-Core（64-HyperThread），基于 X-Engine 的 X-DB 数据库，单实例能够做到近 65 万的 TPS 和 150 万 + 的 QPS。

##### AIOPs
CloudDBA：做到了所有的 SQL 打标、采集、追踪和智能化分析，通过全量的数据收集，辅以机器学习的算法，今年的双 11 人力投入，相对于之前有了极大的降低。

DBPaaS：数据库监控也从早期的分钟级一路进化到现在的秒级，秒级监控，能够发现毫秒级的抖动。

EagleEye：鹰眼，分布式调用跟踪系统。

基于真实线上部署的全链路压测平台。

#### 个人技术发展
>一直到现在我都是这样要求团队的同学的，要关注 CPU、内存、外存、网络的实现架构，要用软硬件 Co-Design 的思维来指导架构设计和开发，这也是 X-DB 高性能的源泉。

案例：我在网易开发 TNT 引擎的时候，遇到过一个十分诡异的性能问题。引擎在高负载下会出现诡异的性能抖动，个别系统 mutex 偶尔会出现调度异常。在开发数据库引擎的时候，为了追求更高的效率，一般都会自己实现 mutex，但是多位同学都 Review 了加锁和释放锁的代码，逻辑均没有异常。

排查陷入了僵局，既然逻辑没问题，那就只可能是执行的问题了，我那时候刚做过 CPU 架构的分享，突然就想到 X86 CPU 虽然很少乱序，但也是有 LoadStore 乱序的可能性的。我就带着这个可能重新 Review 了代码，发现了在 unlock 的实现逻辑中对于 lockword 的写和 waiters 的读有可能存在乱序，导致没有调用唤醒落地，从而出现 mutex 调度异常。